"""
Comprehensive ablation studies for JioPay RAG system.
Evaluates the impact of different chunking strategies, embedding models, and retrieval parameters.
"""
import json
import time
import pandas as pd
from typing import List, Dict, Any, Tuple
from pathlib import Path
from dataclasses import dataclass
from loguru import logger
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

from src.config import get_settings
from src.rag.simple_rag_pipeline import RAGPipeline, RAGResponse
from src.vector_store.vector_manager import VectorStoreManager

settings = get_settings()


@dataclass
class AblationResult:
    """Result from an ablation study experiment."""
    experiment_name: str
    configuration: Dict[str, Any]
    metrics: Dict[str, float]
    responses: List[Dict[str, Any]]
    timestamp: str


class RAGAblationStudy:
    """Comprehensive ablation study for RAG system."""
    
    def __init__(self):
        """Initialize the ablation study."""
        logger.info("ðŸ”„ Initializing RAG Ablation Study...")
        
        self.rag_pipeline = RAGPipeline()
        self.test_queries = self._load_test_queries()
        self.results = []
        
        logger.info("âœ… Ablation study initialized")
    
    def _load_test_queries(self) -> List[Dict[str, Any]]:
        """Load test queries for evaluation."""
        return [
            {
                "query": "How do I integrate JioPay with my website?",
                "expected_topics": ["integration", "api", "sdk", "website"],
                "category": "integration"
            },
            {
                "query": "What are the pricing plans for JioPay Business?",
                "expected_topics": ["pricing", "plans", "fees", "cost"],
                "category": "pricing"
            },
            {
                "query": "What security features does JioPay offer?",
                "expected_topics": ["security", "encryption", "tokenization", "compliance"],
                "category": "security"
            },
            {
                "query": "How do I contact JioPay customer support?",
                "expected_topics": ["support", "contact", "help", "customer service"],
                "category": "support"
            },
            {
                "query": "What payment methods does JioPay support?",
                "expected_topics": ["payment", "methods", "cards", "upi", "wallet"],
                "category": "payments"
            },
            {
                "query": "How do I handle refunds with JioPay?",
                "expected_topics": ["refunds", "returns", "disputes", "chargebacks"],
                "category": "payments"
            },
            {
                "query": "What are the transaction limits?",
                "expected_topics": ["limits", "transactions", "amounts", "restrictions"],
                "category": "limits"
            },
            {
                "query": "How do I set up a JioPay merchant account?",
                "expected_topics": ["account", "setup", "registration", "onboarding"],
                "category": "account"
            }
        ]
    
    def evaluate_retrieval_quality(self, retrieved_chunks: List, query: str, 
                                  expected_topics: List[str]) -> Dict[str, float]:
        """Evaluate the quality of retrieved chunks."""
        if not retrieved_chunks:
            return {
                "precision_at_1": 0.0,
                "precision_at_3": 0.0,
                "precision_at_5": 0.0,
                "recall_at_5": 0.0,
                "avg_relevance_score": 0.0,
                "topic_coverage": 0.0
            }
        
        # Calculate precision at different k values
        precision_at_1 = retrieved_chunks[0].score if len(retrieved_chunks) > 0 else 0.0
        precision_at_3 = np.mean([chunk.score for chunk in retrieved_chunks[:3]]) if len(retrieved_chunks) >= 3 else np.mean([chunk.score for chunk in retrieved_chunks])
        precision_at_5 = np.mean([chunk.score for chunk in retrieved_chunks[:5]]) if len(retrieved_chunks) >= 5 else np.mean([chunk.score for chunk in retrieved_chunks])
        
        # Calculate average relevance score
        avg_relevance_score = np.mean([chunk.score for chunk in retrieved_chunks])
        
        # Calculate topic coverage (simplified)
        chunk_texts = [chunk.text.lower() for chunk in retrieved_chunks]
        all_text = " ".join(chunk_texts)
        topic_coverage = sum(1 for topic in expected_topics if topic.lower() in all_text) / len(expected_topics)
        
        # Calculate recall (simplified - assume we want at least one relevant chunk)
        recall_at_5 = 1.0 if avg_relevance_score > 0.3 else 0.0
        
        return {
            "precision_at_1": precision_at_1,
            "precision_at_3": precision_at_3,
            "precision_at_5": precision_at_5,
            "recall_at_5": recall_at_5,
            "avg_relevance_score": avg_relevance_score,
            "topic_coverage": topic_coverage
        }
    
    def evaluate_response_quality(self, response: str, query: str, 
                                retrieved_chunks: List) -> Dict[str, float]:
        """Evaluate the quality of generated responses."""
        if not response:
            return {
                "response_length": 0.0,
                "has_citations": 0.0,
                "relevance_score": 0.0,
                "completeness_score": 0.0
            }
        
        # Basic response metrics
        response_length = len(response.split())
        has_citations = 1.0 if any(chunk.text[:50] in response for chunk in retrieved_chunks) else 0.0
        
        # Simple relevance scoring (keyword matching)
        query_words = set(query.lower().split())
        response_words = set(response.lower().split())
        relevance_score = len(query_words.intersection(response_words)) / len(query_words) if query_words else 0.0
        
        # Completeness score (based on response length and structure)
        completeness_score = min(1.0, response_length / 50)  # Normalize to 0-1
        
        return {
            "response_length": response_length,
            "has_citations": has_citations,
            "relevance_score": relevance_score,
            "completeness_score": completeness_score
        }
    
    def run_chunking_strategy_ablation(self) -> List[AblationResult]:
        """Compare different chunking strategies."""
        logger.info("ðŸ”¬ Running chunking strategy ablation...")
        
        chunking_strategies = [
            "fixed",
            "semantic", 
            "structural",
            "recursive",
            "llm_based"
        ]
        
        results = []
        
        for strategy in chunking_strategies:
            logger.info(f"Testing chunking strategy: {strategy}")
            
            experiment_results = []
            total_metrics = {
                "precision_at_1": [],
                "precision_at_3": [],
                "precision_at_5": [],
                "recall_at_5": [],
                "avg_relevance_score": [],
                "topic_coverage": [],
                "response_time": [],
                "response_length": [],
                "has_citations": [],
                "relevance_score": [],
                "completeness_score": []
            }
            
            for test_query in self.test_queries:
                try:
                    # Filter collections by chunking strategy
                    collection_name = f"jiopay_bge-small"  # Use BGE-small as baseline
                    
                    start_time = time.time()
                    response = self.rag_pipeline.process_query(
                        test_query["query"],
                        collection_name=collection_name,
                        n_results=5
                    )
                    response_time = time.time() - start_time
                    
                    # Evaluate retrieval quality
                    retrieval_metrics = self.evaluate_retrieval_quality(
                        response.retrieved_chunks,
                        test_query["query"],
                        test_query["expected_topics"]
                    )
                    
                    # Evaluate response quality
                    response_metrics = self.evaluate_response_quality(
                        response.answer,
                        test_query["query"],
                        response.retrieved_chunks
                    )
                    
                    # Combine metrics
                    combined_metrics = {
                        **retrieval_metrics,
                        **response_metrics,
                        "response_time": response_time
                    }
                    
                    # Store results
                    experiment_results.append({
                        "query": test_query["query"],
                        "category": test_query["category"],
                        "response": response.answer,
                        "metrics": combined_metrics
                    })
                    
                    # Aggregate metrics
                    for key, value in combined_metrics.items():
                        total_metrics[key].append(value)
                    
                except Exception as e:
                    logger.error(f"Error testing {strategy} with query '{test_query['query']}': {e}")
                    continue
            
            # Calculate average metrics
            avg_metrics = {
                key: np.mean(values) if values else 0.0 
                for key, values in total_metrics.items()
            }
            
            result = AblationResult(
                experiment_name=f"chunking_strategy_{strategy}",
                configuration={"chunking_strategy": strategy, "embedding_model": "bge-small"},
                metrics=avg_metrics,
                responses=experiment_results,
                timestamp=time.strftime("%Y%m%d_%H%M%S")
            )
            
            results.append(result)
            logger.info(f"âœ… Completed {strategy}: Avg relevance score = {avg_metrics['avg_relevance_score']:.3f}")
        
        return results
    
    def run_embedding_model_ablation(self) -> List[AblationResult]:
        """Compare different embedding models."""
        logger.info("ðŸ”¬ Running embedding model ablation...")
        
        embedding_models = [
            "jiopay_bge-small",
            "jiopay_bge-base", 
            "jiopay_bge-large",
            "jiopay_e5-base",
            "jiopay_e5-large"
        ]
        
        results = []
        
        for model in embedding_models:
            logger.info(f"Testing embedding model: {model}")
            
            experiment_results = []
            total_metrics = {
                "precision_at_1": [],
                "precision_at_3": [],
                "precision_at_5": [],
                "recall_at_5": [],
                "avg_relevance_score": [],
                "topic_coverage": [],
                "response_time": [],
                "response_length": [],
                "has_citations": [],
                "relevance_score": [],
                "completeness_score": []
            }
            
            for test_query in self.test_queries:
                try:
                    start_time = time.time()
                    response = self.rag_pipeline.process_query(
                        test_query["query"],
                        collection_name=model,
                        n_results=5
                    )
                    response_time = time.time() - start_time
                    
                    # Evaluate metrics
                    retrieval_metrics = self.evaluate_retrieval_quality(
                        response.retrieved_chunks,
                        test_query["query"],
                        test_query["expected_topics"]
                    )
                    
                    response_metrics = self.evaluate_response_quality(
                        response.answer,
                        test_query["query"],
                        response.retrieved_chunks
                    )
                    
                    combined_metrics = {
                        **retrieval_metrics,
                        **response_metrics,
                        "response_time": response_time
                    }
                    
                    experiment_results.append({
                        "query": test_query["query"],
                        "category": test_query["category"],
                        "response": response.answer,
                        "metrics": combined_metrics
                    })
                    
                    for key, value in combined_metrics.items():
                        total_metrics[key].append(value)
                    
                except Exception as e:
                    logger.error(f"Error testing {model} with query '{test_query['query']}': {e}")
                    continue
            
            # Calculate averages
            avg_metrics = {
                key: np.mean(values) if values else 0.0 
                for key, values in total_metrics.items()
            }
            
            result = AblationResult(
                experiment_name=f"embedding_model_{model.replace('jiopay_', '')}",
                configuration={"embedding_model": model, "chunking_strategy": "semantic"},
                metrics=avg_metrics,
                responses=experiment_results,
                timestamp=time.strftime("%Y%m%d_%H%M%S")
            )
            
            results.append(result)
            logger.info(f"âœ… Completed {model}: Avg relevance score = {avg_metrics['avg_relevance_score']:.3f}")
        
        return results
    
    def run_retrieval_parameter_ablation(self) -> List[AblationResult]:
        """Compare different retrieval parameters."""
        logger.info("ðŸ”¬ Running retrieval parameter ablation...")
        
        n_results_options = [1, 3, 5, 7, 10]
        results = []
        
        for n_results in n_results_options:
            logger.info(f"Testing n_results: {n_results}")
            
            experiment_results = []
            total_metrics = {
                "precision_at_1": [],
                "precision_at_3": [],
                "precision_at_5": [],
                "recall_at_5": [],
                "avg_relevance_score": [],
                "topic_coverage": [],
                "response_time": [],
                "response_length": [],
                "has_citations": [],
                "relevance_score": [],
                "completeness_score": []
            }
            
            for test_query in self.test_queries:
                try:
                    start_time = time.time()
                    response = self.rag_pipeline.process_query(
                        test_query["query"],
                        collection_name="jiopay_bge-small",
                        n_results=n_results
                    )
                    response_time = time.time() - start_time
                    
                    # Evaluate metrics
                    retrieval_metrics = self.evaluate_retrieval_quality(
                        response.retrieved_chunks,
                        test_query["query"],
                        test_query["expected_topics"]
                    )
                    
                    response_metrics = self.evaluate_response_quality(
                        response.answer,
                        test_query["query"],
                        response.retrieved_chunks
                    )
                    
                    combined_metrics = {
                        **retrieval_metrics,
                        **response_metrics,
                        "response_time": response_time
                    }
                    
                    experiment_results.append({
                        "query": test_query["query"],
                        "category": test_query["category"],
                        "response": response.answer,
                        "metrics": combined_metrics
                    })
                    
                    for key, value in combined_metrics.items():
                        total_metrics[key].append(value)
                    
                except Exception as e:
                    logger.error(f"Error testing n_results={n_results} with query '{test_query['query']}': {e}")
                    continue
            
            # Calculate averages
            avg_metrics = {
                key: np.mean(values) if values else 0.0 
                for key, values in total_metrics.items()
            }
            
            result = AblationResult(
                experiment_name=f"n_results_{n_results}",
                configuration={"n_results": n_results, "embedding_model": "bge-small", "chunking_strategy": "semantic"},
                metrics=avg_metrics,
                responses=experiment_results,
                timestamp=time.strftime("%Y%m%d_%H%M%S")
            )
            
            results.append(result)
            logger.info(f"âœ… Completed n_results={n_results}: Avg relevance score = {avg_metrics['avg_relevance_score']:.3f}")
        
        return results
    
    def run_comprehensive_ablation(self) -> Dict[str, List[AblationResult]]:
        """Run all ablation studies."""
        logger.info("ðŸš€ Starting comprehensive ablation study...")
        
        all_results = {}
        
        # Run chunking strategy ablation
        logger.info("="*60)
        logger.info("PHASE 1: Chunking Strategy Ablation")
        logger.info("="*60)
        chunking_results = self.run_chunking_strategy_ablation()
        all_results["chunking_strategies"] = chunking_results
        
        # Run embedding model ablation
        logger.info("="*60)
        logger.info("PHASE 2: Embedding Model Ablation")
        logger.info("="*60)
        embedding_results = self.run_embedding_model_ablation()
        all_results["embedding_models"] = embedding_results
        
        # Run retrieval parameter ablation
        logger.info("="*60)
        logger.info("PHASE 3: Retrieval Parameter Ablation")
        logger.info("="*60)
        parameter_results = self.run_retrieval_parameter_ablation()
        all_results["retrieval_parameters"] = parameter_results
        
        # Save results
        self.save_results(all_results)
        
        # Generate summary report
        self.generate_summary_report(all_results)
        
        logger.info("ðŸŽ‰ Comprehensive ablation study completed!")
        return all_results
    
    def save_results(self, results: Dict[str, List[AblationResult]]):
        """Save ablation study results."""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # Save detailed results
        detailed_results = {}
        for study_type, study_results in results.items():
            detailed_results[study_type] = []
            for result in study_results:
                detailed_results[study_type].append({
                    "experiment_name": result.experiment_name,
                    "configuration": result.configuration,
                    "metrics": result.metrics,
                    "responses": result.responses,
                    "timestamp": result.timestamp
                })
        
        results_file = Path(settings.evaluation_data_dir) / f"ablation_study_detailed_{timestamp}.json"
        with open(results_file, 'w') as f:
            json.dump(detailed_results, f, indent=2)
        
        logger.info(f"ðŸ’¾ Detailed results saved to: {results_file}")
        
        # Save summary metrics
        summary_metrics = {}
        for study_type, study_results in results.items():
            summary_metrics[study_type] = {}
            for result in study_results:
                summary_metrics[study_type][result.experiment_name] = result.metrics
        
        summary_file = Path(settings.evaluation_data_dir) / f"ablation_study_summary_{timestamp}.json"
        with open(summary_file, 'w') as f:
            json.dump(summary_metrics, f, indent=2)
        
        logger.info(f"ðŸ’¾ Summary metrics saved to: {summary_file}")
    
    def generate_summary_report(self, results: Dict[str, List[AblationResult]]):
        """Generate a summary report of ablation study results."""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        report_file = Path(settings.evaluation_data_dir) / f"ablation_study_report_{timestamp}.md"
        
        with open(report_file, 'w') as f:
            f.write("# RAG System Ablation Study Report\n\n")
            f.write(f"**Generated:** {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # Overall summary
            f.write("## Executive Summary\n\n")
            f.write("This report presents the results of comprehensive ablation studies conducted on the JioPay RAG system. ")
            f.write("The studies evaluate the impact of different chunking strategies, embedding models, and retrieval parameters on system performance.\n\n")
            
            # Chunking strategy results
            if "chunking_strategies" in results:
                f.write("## Chunking Strategy Analysis\n\n")
                f.write("| Strategy | Avg Relevance Score | Precision@5 | Response Time (s) |\n")
                f.write("|----------|-------------------|-------------|-------------------|\n")
                
                for result in results["chunking_strategies"]:
                    strategy = result.configuration["chunking_strategy"]
                    relevance = result.metrics["avg_relevance_score"]
                    precision = result.metrics["precision_at_5"]
                    response_time = result.metrics["response_time"]
                    f.write(f"| {strategy} | {relevance:.3f} | {precision:.3f} | {response_time:.2f} |\n")
                
                f.write("\n")
            
            # Embedding model results
            if "embedding_models" in results:
                f.write("## Embedding Model Analysis\n\n")
                f.write("| Model | Avg Relevance Score | Precision@5 | Response Time (s) |\n")
                f.write("|-------|-------------------|-------------|-------------------|\n")
                
                for result in results["embedding_models"]:
                    model = result.configuration["embedding_model"].replace("jiopay_", "")
                    relevance = result.metrics["avg_relevance_score"]
                    precision = result.metrics["precision_at_5"]
                    response_time = result.metrics["response_time"]
                    f.write(f"| {model} | {relevance:.3f} | {precision:.3f} | {response_time:.2f} |\n")
                
                f.write("\n")
            
            # Retrieval parameter results
            if "retrieval_parameters" in results:
                f.write("## Retrieval Parameter Analysis\n\n")
                f.write("| N Results | Avg Relevance Score | Precision@5 | Response Time (s) |\n")
                f.write("|-----------|-------------------|-------------|-------------------|\n")
                
                for result in results["retrieval_parameters"]:
                    n_results = result.configuration["n_results"]
                    relevance = result.metrics["avg_relevance_score"]
                    precision = result.metrics["precision_at_5"]
                    response_time = result.metrics["response_time"]
                    f.write(f"| {n_results} | {relevance:.3f} | {precision:.3f} | {response_time:.2f} |\n")
                
                f.write("\n")
            
            # Recommendations
            f.write("## Recommendations\n\n")
            f.write("Based on the ablation study results:\n\n")
            
            # Find best performing configurations
            best_chunking = max(results.get("chunking_strategies", []), 
                              key=lambda x: x.metrics["avg_relevance_score"], 
                              default=None)
            best_embedding = max(results.get("embedding_models", []), 
                               key=lambda x: x.metrics["avg_relevance_score"], 
                               default=None)
            best_n_results = max(results.get("retrieval_parameters", []), 
                               key=lambda x: x.metrics["avg_relevance_score"], 
                               default=None)
            
            if best_chunking:
                f.write(f"1. **Best Chunking Strategy:** {best_chunking.configuration['chunking_strategy']} ")
                f.write(f"(Relevance Score: {best_chunking.metrics['avg_relevance_score']:.3f})\n")
            
            if best_embedding:
                f.write(f"2. **Best Embedding Model:** {best_embedding.configuration['embedding_model'].replace('jiopay_', '')} ")
                f.write(f"(Relevance Score: {best_embedding.metrics['avg_relevance_score']:.3f})\n")
            
            if best_n_results:
                f.write(f"3. **Optimal N Results:** {best_n_results.configuration['n_results']} ")
                f.write(f"(Relevance Score: {best_n_results.metrics['avg_relevance_score']:.3f})\n")
            
            f.write("\n")
            f.write("## Methodology\n\n")
            f.write("- **Test Queries:** 8 diverse queries covering integration, pricing, security, support, payments, limits, and account setup\n")
            f.write("- **Evaluation Metrics:** Precision@k, Recall@k, Average Relevance Score, Topic Coverage, Response Quality\n")
            f.write("- **Baseline Model:** BGE-small with semantic chunking\n")
            f.write("- **Response Generation:** Gemini 1.5 Flash with fallback to mock responses\n\n")
        
        logger.info(f"ðŸ“Š Summary report saved to: {report_file}")


def main():
    """Run comprehensive ablation study."""
    logger.info("ðŸš€ Starting RAG System Ablation Study")
    
    # Create evaluation directory
    Path(settings.evaluation_data_dir).mkdir(parents=True, exist_ok=True)
    
    # Initialize and run ablation study
    ablation_study = RAGAblationStudy()
    results = ablation_study.run_comprehensive_ablation()
    
    logger.info("ðŸŽ‰ Ablation study completed successfully!")
    logger.info("ðŸ“Š Check the evaluation directory for detailed results and reports")


if __name__ == "__main__":
    main()
